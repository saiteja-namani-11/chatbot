{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiteja-namani-11/chatbot/blob/main/ChatBot_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Importing the libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "mkk-fhimojPG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 mounting the drive to the colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKJ3cpCqomga",
        "outputId": "73f5bad8-df79-44e5-8c9a-825bc91c9387"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Importing the dataset\n",
        "lines = open('/content/drive/MyDrive/NLP LAB WORKS/project/Chatbot-Seq2Seq-master/Data/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('/content/drive/MyDrive/NLP LAB WORKS/project/Chatbot-Seq2Seq-master/Data/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "# reads the files including the encoding of the file.\n",
        "# spliting is performed as its a big text file which returns the list of strings"
      ],
      "metadata": {
        "id": "V1Q8N-9xop2g"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Creating a dictionary that maps each line and its id\n",
        "id2line = {}# creatiing an empty dictionary\n",
        "for line in lines: # a loop that iterates over each line in lines \n",
        "    _line = line.split(' +++$+++ ') # splits the line and stored in _line\n",
        "    if len(_line) == 5: # checks for the length of the list is equal to 5\n",
        "        id2line[_line[0]] = _line[4] # maps each line id to its corresponding line of dialog in the id2line dictionry"
      ],
      "metadata": {
        "id": "Sk-x3EmkosoI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Creating a list of all of the conversations\n",
        "conversations_ids = [] # empty list which stores the conversation ids parsed from the data.\n",
        "for conversation in conversations[:-1]: # looping the conversations from the data \n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\") # removing all the quotes spaces, squared brackets from the conversation\n",
        "    conversations_ids.append(_conversation.split(',')) # spliting the string by commas and appending it into the conversation ids."
      ],
      "metadata": {
        "id": "13brvNZGouyq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Getting separately the questions and the answers\n",
        "questions = []\n",
        "answers = []# creating 2 empty lists\n",
        "for conversation in conversations_ids: # looping on the conversation ids.\n",
        "    for i in range(len(conversation) - 1):# another loop over the ids from start to last but one id\n",
        "        questions.append(id2line[conversation[i]]) \n",
        "        answers.append(id2line[conversation[i+1]])\n",
        "        # for every itteration the ith element is considered as question and i+1 as the answer."
      ],
      "metadata": {
        "id": "13CL75MPo4XO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Doing a first cleaning of the texts\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;$:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text\n",
        "    #removing all the punctuations and special characters and converts the text to lower case."
      ],
      "metadata": {
        "id": "IomWAcQto9ao"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Cleaning the questions\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "    # storing the clean text into a new list"
      ],
      "metadata": {
        "id": "4asdxT0rpAUD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Cleaning the answers\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))\n",
        "    # storing the clean text into a new list"
      ],
      "metadata": {
        "id": "QV_gHrQnpCns"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Filtering out the questions and answers that are too short or too long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if 2 <= len(question.split()) <= 25:# checing for the length greater than or equal to 2 and less than or equal to25\n",
        "        short_questions.append(question)\n",
        "        short_answers.append(clean_answers[i])\n",
        "    i += 1\n",
        "    # if the condition is passed the corresponding questions and answers are added to the empty list.\n",
        "clean_questions = []\n",
        "clean_answers = []\n",
        "i = 0\n",
        "for answer in short_answers:\n",
        "    if 2 <= len(answer.split()) <= 25:\n",
        "        clean_answers.append(answer)\n",
        "        clean_questions.append(short_questions[i])\n",
        "    i += 1\n",
        "    # the same condition the code is added to the clean questions and clean answers."
      ],
      "metadata": {
        "id": "wRxwttUrpFpf"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11 Creating a dictionary that maps each word to its number of occurrences\n",
        "word2count = {} # creating an empty dictionary\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "            # maps each word in the cleaned question to its number of occurance\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "            # maps each word in the cleaned answers to its number of occurance"
      ],
      "metadata": {
        "id": "URCoWp3cpOym"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
        "threshold_questions = 15 # 15 minimum number of occurances a word should have to be included in the dictionary of questions and answers.\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "        # the mapping happens for each question and the total nnumber of words that meet the threshold are collected.\n",
        "threshold_answers = 15\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "        # the mapping happens for each answer and the total nnumber of words that meet the threshold are collected."
      ],
      "metadata": {
        "id": "_xjTXgqupRdu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13 Adding the last tokens to these two dictionaries\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']# new tokents are stored in the tokens variable.\n",
        "# which include padding of hte sequence which is making sure of the length,end of sequence marker.\n",
        "#<out> is used for rare words that are not included int he dictionary\n",
        "#and the start of the sequence marker.\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) + 1\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) + 1\n",
        "# all of these markers are added to both the question and answer words."
      ],
      "metadata": {
        "id": "eLvD7Cv1pcai"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14 Creating the inverse dictionary of the answerswords2int dictionary\n",
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}\n",
        "# the new dictionary has the inverse of what answerswords2int had."
      ],
      "metadata": {
        "id": "Xz48KJdVpdMy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15 Adding the End Of String token to the end of every answer\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += ' <EOS>'\n",
        "    # adding an end of sequence after every answer in clean answers."
      ],
      "metadata": {
        "id": "KIgUcgVbpdJD"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16 Translating all the questions and the answers into integers\n",
        "# and Replacing all the words that were filtered out by <OUT> \n",
        "questions_into_int = []\n",
        "for question in clean_questions:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "    # the inteeger representation of the questions are stored in the questions_into_int\n",
        "answers_into_int = []\n",
        "for answer in clean_answers:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)\n",
        "    # the inteeger representation of the answers are stored in the answers_into_int"
      ],
      "metadata": {
        "id": "2CQKxWPrpgk9"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "fU2ykCX1Ts1v"
      },
      "outputs": [],
      "source": [
        "#17 Sorting questions and answers by the length of questions\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = [] # created empty lists.\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])\n",
        "# this block of code actually sorts the questions based on their length of the question."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Reduce the threshold for word occurrence\n",
        "threshold_questions = 10\n",
        "threshold_answers = 10\n",
        "# the minimum threshold is dropped from 15 to 10"
      ],
      "metadata": {
        "id": "ikEiAgF5itgt"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19\n",
        "#Set the parameters for the model\n",
        "input_vocab_size = len(questionswords2int) + 1 \n",
        "# the input vocab size is set to the question size +1\n",
        "output_vocab_size = len(answerswords2int) +1\n",
        "#the output vocab size is set to the answer size +1\n",
        "max_length = 25\n"
      ],
      "metadata": {
        "id": "Jl2BH9tappQs"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Define the encoder input\n",
        "encoder_inputs = Input(shape=(None,))# indicates the model can handle variable length sequence.\n",
        "encoder_embedding = Embedding(input_vocab_size, 128)(encoder_inputs)\n",
        "# creating an embedded layer to map the input sequence to a denser vector space with 128\n",
        "encoder_outputs, state_h, state_c = LSTM(128, return_state=True)(encoder_embedding)\n",
        "# creating an LSTM layer with 128 units that reeturns the hidden state and the cell state.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "eKaek4EKprcB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21 Define the decoder input\n",
        "decoder_inputs = Input(shape=(None,)) #indicates the model can handle variable length sequence.\n",
        "decoder_embedding = Embedding(output_vocab_size, 128)(decoder_inputs) \n",
        "#Creates an embedding layer for the decoder input.\n",
        "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
        "# defines an LSTM layer for the decoder with 128 units \n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)"
      ],
      "metadata": {
        "id": "8qkPbQ4npsLj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Define the output layerof the decoder which uses dense layers with softmax activation function.\n",
        "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
        "decoder_outputs = Dense(output_vocab_size, activation='softmax')(decoder_outputs)"
      ],
      "metadata": {
        "id": "SEyUU-Owpxrf"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model using the adams optimizer  and a categorical cross entropy loss.\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "jlHQXunyTvhA"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Split the dataset into training and testing sets\n",
        "import random\n",
        "\n",
        "sample_size = 10000  # Adjust this value according to your needs\n",
        "sample_indices = random.sample(range(len(sorted_clean_questions)), sample_size)\n",
        "# sampling 10000 off the sorted clean questions.\n",
        "\n",
        "sample_questions = [sorted_clean_questions[i] for i in sample_indices]\n",
        "sample_answers = [sorted_clean_answers[i] for i in sample_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_questions, sample_answers, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "uPZvhYHgpr94"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25 Prepare the data for training\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences #using to pad sequences to the same length.\n",
        "from tensorflow.keras.utils import to_categorical # using to get the onehot encoded\n",
        "#  Free up memory\n",
        "del lines, conversations, questions, answers, word2count\n",
        "# deleting variable line to free up memory\n",
        "gc.collect()# forces the garbage collector to free up memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYcIE5Y4p6i-",
        "outputId": "39cacb98-3814-441e-97a4-c518b658b05b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6369"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#26 \n",
        "def preprocess_data(questions, answers, max_length):\n",
        "    encoder_input_data = pad_sequences(questions, maxlen=max_length, padding='post')#pading zeros at the end of all the sequences have same length\n",
        "    decoder_input_data = pad_sequences(answers, maxlen=max_length, padding='post')# this is obtained by padding the answers seq with zero\n",
        "    decoder_target_data = np.zeros((len(answers), max_length, output_vocab_size), dtype='float32') # creating ana array of zeros\n",
        "    \n",
        "    for i, answer in enumerate(answers):\n",
        "        for t, word_int in enumerate(answer):\n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, word_int] = 1.0\n",
        "                # the loop iterates over answers and if its a match for each word a alue of 1.0 is set correspondingly in decoder_target_data\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
        "#\n",
        "encoder_input_train, decoder_input_train, decoder_target_train = preprocess_data(X_train, y_train, max_length)\n",
        "encoder_input_test, decoder_input_test, decoder_target_test = preprocess_data(X_test, y_test, max_length)\n",
        "# applying the preprocess_data function to training and testing of the data"
      ],
      "metadata": {
        "id": "YJxZHjmXfHl0"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27  Train the model with a smaller batch size and gradient clipping\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "optimizer = Adam(clipnorm=1.0) #optimizer is set as the adams optimmizer\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
        "\n",
        "model.fit([encoder_input_train, decoder_input_train], decoder_target_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "# training the model that we have created."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY-UV3gMfK_7",
        "outputId": "10bdd75e-2988-4a03-96d5-feb70fb7ca9a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "254/254 [==============================] - 101s 376ms/step - loss: 2.1862 - val_loss: 2.0854\n",
            "Epoch 2/50\n",
            "254/254 [==============================] - 91s 357ms/step - loss: 2.0748 - val_loss: 2.0735\n",
            "Epoch 3/50\n",
            "254/254 [==============================] - 91s 358ms/step - loss: 2.0620 - val_loss: 2.0660\n",
            "Epoch 4/50\n",
            "254/254 [==============================] - 90s 355ms/step - loss: 2.0514 - val_loss: 2.0609\n",
            "Epoch 5/50\n",
            "254/254 [==============================] - 90s 353ms/step - loss: 2.0453 - val_loss: 2.0590\n",
            "Epoch 6/50\n",
            "254/254 [==============================] - 90s 353ms/step - loss: 2.0385 - val_loss: 2.0588\n",
            "Epoch 7/50\n",
            "254/254 [==============================] - 89s 351ms/step - loss: 2.0332 - val_loss: 2.0517\n",
            "Epoch 8/50\n",
            "254/254 [==============================] - 89s 351ms/step - loss: 2.0273 - val_loss: 2.0531\n",
            "Epoch 9/50\n",
            "254/254 [==============================] - 89s 351ms/step - loss: 2.0222 - val_loss: 2.0410\n",
            "Epoch 10/50\n",
            "254/254 [==============================] - 90s 354ms/step - loss: 2.0175 - val_loss: 2.0403\n",
            "Epoch 11/50\n",
            "254/254 [==============================] - 93s 366ms/step - loss: 2.0139 - val_loss: 2.0434\n",
            "Epoch 12/50\n",
            "254/254 [==============================] - 90s 352ms/step - loss: 2.0098 - val_loss: 2.0387\n",
            "Epoch 13/50\n",
            "254/254 [==============================] - 89s 353ms/step - loss: 2.0057 - val_loss: 2.0368\n",
            "Epoch 14/50\n",
            "254/254 [==============================] - 89s 352ms/step - loss: 2.0016 - val_loss: 2.0371\n",
            "Epoch 15/50\n",
            "254/254 [==============================] - 90s 354ms/step - loss: 1.9969 - val_loss: 2.0298\n",
            "Epoch 16/50\n",
            "254/254 [==============================] - 90s 355ms/step - loss: 1.9934 - val_loss: 2.0310\n",
            "Epoch 17/50\n",
            "254/254 [==============================] - 90s 353ms/step - loss: 1.9908 - val_loss: 2.0348\n",
            "Epoch 18/50\n",
            "254/254 [==============================] - 89s 351ms/step - loss: 1.9885 - val_loss: 2.0345\n",
            "Epoch 19/50\n",
            "254/254 [==============================] - 90s 354ms/step - loss: 1.9835 - val_loss: 2.0309\n",
            "Epoch 20/50\n",
            "254/254 [==============================] - 93s 366ms/step - loss: 1.9805 - val_loss: 2.0371\n",
            "Epoch 21/50\n",
            "254/254 [==============================] - 90s 354ms/step - loss: 1.9771 - val_loss: 2.0321\n",
            "Epoch 22/50\n",
            "254/254 [==============================] - 93s 365ms/step - loss: 1.9741 - val_loss: 2.0326\n",
            "Epoch 23/50\n",
            "254/254 [==============================] - 92s 363ms/step - loss: 1.9706 - val_loss: 2.0321\n",
            "Epoch 24/50\n",
            "254/254 [==============================] - 93s 365ms/step - loss: 1.9670 - val_loss: 2.0279\n",
            "Epoch 25/50\n",
            "254/254 [==============================] - 92s 362ms/step - loss: 1.9640 - val_loss: 2.0284\n",
            "Epoch 26/50\n",
            "254/254 [==============================] - 90s 356ms/step - loss: 1.9606 - val_loss: 2.0285\n",
            "Epoch 27/50\n",
            "254/254 [==============================] - 90s 355ms/step - loss: 1.9568 - val_loss: 2.0262\n",
            "Epoch 28/50\n",
            "254/254 [==============================] - 89s 351ms/step - loss: 1.9518 - val_loss: 2.0278\n",
            "Epoch 29/50\n",
            "254/254 [==============================] - 93s 365ms/step - loss: 1.9435 - val_loss: 2.0201\n",
            "Epoch 30/50\n",
            "254/254 [==============================] - 91s 356ms/step - loss: 1.9302 - val_loss: 2.0034\n",
            "Epoch 31/50\n",
            "254/254 [==============================] - 90s 353ms/step - loss: 1.9099 - val_loss: 1.9874\n",
            "Epoch 32/50\n",
            "254/254 [==============================] - 89s 351ms/step - loss: 1.8906 - val_loss: 1.9742\n",
            "Epoch 33/50\n",
            "254/254 [==============================] - 91s 359ms/step - loss: 1.8742 - val_loss: 1.9653\n",
            "Epoch 34/50\n",
            "254/254 [==============================] - 90s 353ms/step - loss: 1.8569 - val_loss: 1.9525\n",
            "Epoch 35/50\n",
            "254/254 [==============================] - 90s 354ms/step - loss: 1.8491 - val_loss: 1.9400\n",
            "Epoch 36/50\n",
            "254/254 [==============================] - 90s 354ms/step - loss: 1.8283 - val_loss: 1.9283\n",
            "Epoch 37/50\n",
            "254/254 [==============================] - 89s 353ms/step - loss: 1.8112 - val_loss: 1.9146\n",
            "Epoch 38/50\n",
            "254/254 [==============================] - 89s 350ms/step - loss: 1.7961 - val_loss: 1.9085\n",
            "Epoch 39/50\n",
            "254/254 [==============================] - 89s 349ms/step - loss: 1.7796 - val_loss: 1.8903\n",
            "Epoch 40/50\n",
            "254/254 [==============================] - 92s 363ms/step - loss: 1.7609 - val_loss: 1.8832\n",
            "Epoch 41/50\n",
            "254/254 [==============================] - 89s 351ms/step - loss: 1.7451 - val_loss: 1.8757\n",
            "Epoch 42/50\n",
            "254/254 [==============================] - 94s 371ms/step - loss: 1.7272 - val_loss: 1.8682\n",
            "Epoch 43/50\n",
            "254/254 [==============================] - 88s 346ms/step - loss: 1.7103 - val_loss: 1.8600\n",
            "Epoch 44/50\n",
            "254/254 [==============================] - 86s 338ms/step - loss: 1.6934 - val_loss: 1.8562\n",
            "Epoch 45/50\n",
            "254/254 [==============================] - 87s 344ms/step - loss: 1.6782 - val_loss: 1.8528\n",
            "Epoch 46/50\n",
            "254/254 [==============================] - 88s 347ms/step - loss: 1.6637 - val_loss: 1.8514\n",
            "Epoch 47/50\n",
            "254/254 [==============================] - 88s 348ms/step - loss: 1.6501 - val_loss: 1.8468\n",
            "Epoch 48/50\n",
            "254/254 [==============================] - 88s 346ms/step - loss: 1.6381 - val_loss: 1.8492\n",
            "Epoch 49/50\n",
            "254/254 [==============================] - 88s 346ms/step - loss: 1.6275 - val_loss: 1.8465\n",
            "Epoch 50/50\n",
            "254/254 [==============================] - 88s 348ms/step - loss: 1.6153 - val_loss: 1.8487\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f17881dd2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#28  Test the model and evaluate its performance\n",
        "loss = model.evaluate([encoder_input_test, decoder_input_test], decoder_target_test, batch_size=batch_size)\n",
        "print('Test loss:', loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuIBVXytnW77",
        "outputId": "e1b30253-0d60-4783-f76a-d050357d3b0b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 4s 112ms/step - loss: 1.8815\n",
            "Test loss: 1.8814653158187866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#29 (Optional) Save the trained modelb\n",
        "model.save('seq2seq_chatbot.h5')"
      ],
      "metadata": {
        "id": "tfjS2VvvnZ8I"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30 Load the saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('seq2seq_chatbot.h5')\n",
        "\n",
        "# Define functions for encoding and decoding input/output sequences\n",
        "# the elow coode encodes the the input sequence by mapping each word to its integer representation.\n",
        "def encode_input_sequence(input_sequence, word2int):\n",
        "    encoded_sequence = [word2int['<SOS>']]\n",
        "    for word in input_sequence.split():\n",
        "        if word in word2int:\n",
        "            encoded_sequence.append(word2int[word])\n",
        "            # if the word is found the word is added to the encoded sequence.\n",
        "        else:\n",
        "            encoded_sequence.append(word2int['<OUT>'])\n",
        "            # if the condition is unsatisfied then it is mapped to the <out> tooken\n",
        "    return encoded_sequence\n",
        "\n",
        "# the similar approach is used for the decoder sequence as well.\n",
        "def decode_output_sequence(output_sequence, int2word):\n",
        "    decoded_sequence = ''\n",
        "    for integer in output_sequence:\n",
        "        if integer > 0:\n",
        "            decoded_sequence += int2word[integer] + ' '\n",
        "    return decoded_sequence\n",
        "\n",
        "\n",
        "# Define a function to generate a response from the chatbot\n",
        "def generate_response(input_text):\n",
        "    # Encode the input sequence\n",
        "    input_sequence = encode_input_sequence(input_text, questionswords2int)\n",
        "    # Pad the input sequence\n",
        "    padded_input_sequence = pad_sequences([input_sequence], maxlen=max_length, padding='post')\n",
        "    # Predict the output sequence\n",
        "    output_sequence = model.predict([padded_input_sequence, np.zeros((1, max_length))])[0]\n",
        "    # Decode the output sequence\n",
        "    decoded_output_sequence = decode_output_sequence(np.argmax(output_sequence, axis=1), answersints2word)\n",
        "    # Remove the <EOS> token from the output sequence\n",
        "    if '<EOS>' in decoded_output_sequence:\n",
        "        decoded_output_sequence = decoded_output_sequence[:decoded_output_sequence.index('<EOS>')]\n",
        "    return decoded_output_sequence\n",
        "\n",
        "# Start the chat\n",
        "while True:\n",
        "    input_text = input('You: ')\n",
        "    if input_text.lower() == 'quit':\n",
        "        break\n",
        "    response = generate_response(input_text)\n",
        "    print('Bot:', response)\n"
      ],
      "metadata": {
        "id": "QVlpSoogttjV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}