{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiteja-namani-11/chatbot/blob/main/ChatBot_Seq2Seq2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time"
      ],
      "metadata": {
        "id": "mkk-fhimojPG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKJ3cpCqomga",
        "outputId": "4d1c4485-d07f-4cf3-fae0-56f29f61d23f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "lines = open('/content/drive/MyDrive/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('/content/drive/MyDrive/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "metadata": {
        "id": "V1Q8N-9xop2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary that maps each line and its id\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ],
      "metadata": {
        "id": "Sk-x3EmkosoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all of the conversations\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))"
      ],
      "metadata": {
        "id": "13brvNZGouyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting separately the questions and the answers\n",
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "    for i in range(len(conversation) - 1):\n",
        "        questions.append(id2line[conversation[i]])\n",
        "        answers.append(id2line[conversation[i+1]])"
      ],
      "metadata": {
        "id": "13CL75MPo4XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing a first cleaning of the texts\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;$:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "IomWAcQto9ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the questions\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))"
      ],
      "metadata": {
        "id": "4asdxT0rpAUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the answers\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "metadata": {
        "id": "QV_gHrQnpCns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering out the questions and answers that are too short or too long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if 2 <= len(question.split()) <= 25:\n",
        "        short_questions.append(question)\n",
        "        short_answers.append(clean_answers[i])\n",
        "    i += 1\n",
        "clean_questions = []\n",
        "clean_answers = []\n",
        "i = 0\n",
        "for answer in short_answers:\n",
        "    if 2 <= len(answer.split()) <= 25:\n",
        "        clean_answers.append(answer)\n",
        "        clean_questions.append(short_questions[i])\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "wRxwttUrpFpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary that maps each word to its number of occurrences\n",
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1"
      ],
      "metadata": {
        "id": "URCoWp3cpOym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
        "threshold_questions = 15\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "threshold_answers = 15\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1"
      ],
      "metadata": {
        "id": "_xjTXgqupRdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the last tokens to these two dictionaries\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) + 1\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) + 1"
      ],
      "metadata": {
        "id": "eLvD7Cv1pcai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the inverse dictionary of the answerswords2int dictionary\n",
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
      ],
      "metadata": {
        "id": "Xz48KJdVpdMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the End Of String token to the end of every answer\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += ' <EOS>'"
      ],
      "metadata": {
        "id": "KIgUcgVbpdJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translating all the questions and the answers into integers\n",
        "# and Replacing all the words that were filtered out by <OUT> \n",
        "questions_into_int = []\n",
        "for question in clean_questions:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "answers_into_int = []\n",
        "for answer in clean_answers:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)"
      ],
      "metadata": {
        "id": "2CQKxWPrpgk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU2ykCX1Ts1v"
      },
      "outputs": [],
      "source": [
        "# Sorting questions and answers by the length of questions\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "5tkL-36_do14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Reduce the threshold for word occurrence\n",
        "threshold_questions = 10\n",
        "threshold_answers = 10"
      ],
      "metadata": {
        "id": "ikEiAgF5itgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Seq2Seq model\n",
        "\n",
        "# Set the parameters for the model\n",
        "input_vocab_size = len(questionswords2int) + 1\n",
        "output_vocab_size = len(answerswords2int) + 1\n",
        "max_length = 25\n"
      ],
      "metadata": {
        "id": "Jl2BH9tappQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the encoder input\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_vocab_size, 128)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = LSTM(128, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "eKaek4EKprcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the decoder input\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(output_vocab_size, 128)(decoder_inputs)\n",
        "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)"
      ],
      "metadata": {
        "id": "8qkPbQ4npsLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output layer\n",
        "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
        "decoder_outputs = Dense(output_vocab_size, activation='softmax')(decoder_outputs)"
      ],
      "metadata": {
        "id": "SEyUU-Owpxrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "jlHQXunyTvhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "import random\n",
        "\n",
        "sample_size = 10000  # Adjust this value according to your needs\n",
        "sample_indices = random.sample(range(len(sorted_clean_questions)), sample_size)\n",
        "\n",
        "sample_questions = [sorted_clean_questions[i] for i in sample_indices]\n",
        "sample_answers = [sorted_clean_answers[i] for i in sample_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_questions, sample_answers, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "uPZvhYHgpr94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for training\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#  Free up memory\n",
        "del lines, conversations, questions, answers, word2count\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYcIE5Y4p6i-",
        "outputId": "bc6eadfc-b1e5-4443-d23d-9d46089d3c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7116"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(questions, answers, max_length):\n",
        "    encoder_input_data = pad_sequences(questions, maxlen=max_length, padding='post')\n",
        "    decoder_input_data = pad_sequences(answers, maxlen=max_length, padding='post')\n",
        "    decoder_target_data = np.zeros((len(answers), max_length, output_vocab_size), dtype='float32')\n",
        "    \n",
        "    for i, answer in enumerate(answers):\n",
        "        for t, word_int in enumerate(answer):\n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, word_int] = 1.0\n",
        "                \n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
        "\n",
        "encoder_input_train, decoder_input_train, decoder_target_train = preprocess_data(X_train, y_train, max_length)\n",
        "encoder_input_test, decoder_input_test, decoder_target_test = preprocess_data(X_test, y_test, max_length)"
      ],
      "metadata": {
        "id": "YJxZHjmXfHl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Train the model with a smaller batch size and gradient clipping\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "optimizer = Adam(clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
        "\n",
        "model.fit([encoder_input_train, decoder_input_train], decoder_target_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY-UV3gMfK_7",
        "outputId": "2e2cbc42-2e78-48ab-aeb0-3f628f8847b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "254/254 [==============================] - 86s 322ms/step - loss: 2.1896 - val_loss: 2.0367\n",
            "Epoch 2/50\n",
            "254/254 [==============================] - 78s 308ms/step - loss: 2.0810 - val_loss: 2.0304\n",
            "Epoch 3/50\n",
            "254/254 [==============================] - 77s 305ms/step - loss: 2.0485 - val_loss: 1.9760\n",
            "Epoch 4/50\n",
            "254/254 [==============================] - 78s 309ms/step - loss: 2.0097 - val_loss: 1.9535\n",
            "Epoch 5/50\n",
            "254/254 [==============================] - 80s 313ms/step - loss: 1.9930 - val_loss: 1.9474\n",
            "Epoch 6/50\n",
            "254/254 [==============================] - 78s 309ms/step - loss: 1.9831 - val_loss: 1.9433\n",
            "Epoch 7/50\n",
            "254/254 [==============================] - 79s 311ms/step - loss: 1.9762 - val_loss: 1.9380\n",
            "Epoch 8/50\n",
            "254/254 [==============================] - 81s 318ms/step - loss: 1.9708 - val_loss: 1.9359\n",
            "Epoch 9/50\n",
            "254/254 [==============================] - 79s 311ms/step - loss: 1.9663 - val_loss: 1.9374\n",
            "Epoch 10/50\n",
            "254/254 [==============================] - 82s 322ms/step - loss: 1.9626 - val_loss: 1.9336\n",
            "Epoch 11/50\n",
            "254/254 [==============================] - 80s 316ms/step - loss: 1.9566 - val_loss: 1.9226\n",
            "Epoch 12/50\n",
            "254/254 [==============================] - 81s 318ms/step - loss: 1.9359 - val_loss: 1.9082\n",
            "Epoch 13/50\n",
            "254/254 [==============================] - 80s 313ms/step - loss: 1.9110 - val_loss: 1.8937\n",
            "Epoch 14/50\n",
            "254/254 [==============================] - 79s 313ms/step - loss: 1.8889 - val_loss: 1.8805\n",
            "Epoch 15/50\n",
            "254/254 [==============================] - 82s 322ms/step - loss: 1.8680 - val_loss: 1.8699\n",
            "Epoch 16/50\n",
            "254/254 [==============================] - 79s 313ms/step - loss: 1.8508 - val_loss: 1.8611\n",
            "Epoch 17/50\n",
            "254/254 [==============================] - 79s 313ms/step - loss: 1.8346 - val_loss: 1.8537\n",
            "Epoch 18/50\n",
            "254/254 [==============================] - 81s 318ms/step - loss: 1.8196 - val_loss: 1.8481\n",
            "Epoch 19/50\n",
            "254/254 [==============================] - 77s 302ms/step - loss: 1.8040 - val_loss: 1.8408\n",
            "Epoch 20/50\n",
            "254/254 [==============================] - 79s 312ms/step - loss: 1.7873 - val_loss: 1.8340\n",
            "Epoch 21/50\n",
            "254/254 [==============================] - 82s 322ms/step - loss: 1.7712 - val_loss: 1.8297\n",
            "Epoch 22/50\n",
            "254/254 [==============================] - 80s 315ms/step - loss: 1.7549 - val_loss: 1.8181\n",
            "Epoch 23/50\n",
            "254/254 [==============================] - 80s 316ms/step - loss: 1.7368 - val_loss: 1.8129\n",
            "Epoch 24/50\n",
            "254/254 [==============================] - 81s 319ms/step - loss: 1.7199 - val_loss: 1.8103\n",
            "Epoch 25/50\n",
            "254/254 [==============================] - 79s 310ms/step - loss: 1.7046 - val_loss: 1.8020\n",
            "Epoch 26/50\n",
            "254/254 [==============================] - 82s 321ms/step - loss: 1.6883 - val_loss: 1.7933\n",
            "Epoch 27/50\n",
            "254/254 [==============================] - 79s 313ms/step - loss: 1.6716 - val_loss: 1.7886\n",
            "Epoch 28/50\n",
            "254/254 [==============================] - 82s 323ms/step - loss: 1.6575 - val_loss: 1.7905\n",
            "Epoch 29/50\n",
            "254/254 [==============================] - 79s 312ms/step - loss: 1.6439 - val_loss: 1.7925\n",
            "Epoch 30/50\n",
            "254/254 [==============================] - 79s 312ms/step - loss: 1.6315 - val_loss: 1.7902\n",
            "Epoch 31/50\n",
            "254/254 [==============================] - 81s 319ms/step - loss: 1.6181 - val_loss: 1.7907\n",
            "Epoch 32/50\n",
            "254/254 [==============================] - 79s 313ms/step - loss: 1.6058 - val_loss: 1.7955\n",
            "Epoch 33/50\n",
            "254/254 [==============================] - 81s 320ms/step - loss: 1.5936 - val_loss: 1.8003\n",
            "Epoch 34/50\n",
            "254/254 [==============================] - 79s 311ms/step - loss: 1.5841 - val_loss: 1.8034\n",
            "Epoch 35/50\n",
            "254/254 [==============================] - 83s 325ms/step - loss: 1.5706 - val_loss: 1.8124\n",
            "Epoch 36/50\n",
            "254/254 [==============================] - 79s 313ms/step - loss: 1.5595 - val_loss: 1.8205\n",
            "Epoch 37/50\n",
            "254/254 [==============================] - 82s 323ms/step - loss: 1.5497 - val_loss: 1.8284\n",
            "Epoch 38/50\n",
            "254/254 [==============================] - 80s 316ms/step - loss: 1.5385 - val_loss: 1.8438\n",
            "Epoch 39/50\n",
            "254/254 [==============================] - 82s 325ms/step - loss: 1.5282 - val_loss: 1.8545\n",
            "Epoch 40/50\n",
            "254/254 [==============================] - 81s 319ms/step - loss: 1.5182 - val_loss: 1.8661\n",
            "Epoch 41/50\n",
            "254/254 [==============================] - 82s 324ms/step - loss: 1.5081 - val_loss: 1.8798\n",
            "Epoch 42/50\n",
            "254/254 [==============================] - 79s 313ms/step - loss: 1.4981 - val_loss: 1.8912\n",
            "Epoch 43/50\n",
            "254/254 [==============================] - 82s 321ms/step - loss: 1.4908 - val_loss: 1.9000\n",
            "Epoch 44/50\n",
            "254/254 [==============================] - 81s 320ms/step - loss: 1.4798 - val_loss: 1.9122\n",
            "Epoch 45/50\n",
            "254/254 [==============================] - 88s 345ms/step - loss: 1.4691 - val_loss: 1.9205\n",
            "Epoch 46/50\n",
            "254/254 [==============================] - 79s 312ms/step - loss: 1.4603 - val_loss: 1.9311\n",
            "Epoch 47/50\n",
            "254/254 [==============================] - 81s 321ms/step - loss: 1.4516 - val_loss: 1.9457\n",
            "Epoch 48/50\n",
            "254/254 [==============================] - 79s 311ms/step - loss: 1.4419 - val_loss: 1.9558\n",
            "Epoch 49/50\n",
            "254/254 [==============================] - 79s 311ms/step - loss: 1.4317 - val_loss: 1.9718\n",
            "Epoch 50/50\n",
            "254/254 [==============================] - 82s 323ms/step - loss: 1.4217 - val_loss: 1.9862\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7c2811f3a0>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Test the model and evaluate its performance\n",
        "loss = model.evaluate([encoder_input_test, decoder_input_test], decoder_target_test, batch_size=batch_size)\n",
        "print('Test loss:', loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuIBVXytnW77",
        "outputId": "de885056-13d7-445c-dbe3-3d8469ebc65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 3s 103ms/step - loss: 2.0193\n",
            "Test loss: 2.0192832946777344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  (Optional) Save the trained model\n",
        "model.save('seq2seq_chatbot.h5')"
      ],
      "metadata": {
        "id": "tfjS2VvvnZ8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('seq2seq_chatbot.h5')\n",
        "\n",
        "# Define functions for encoding and decoding input/output sequences\n",
        "def encode_input_sequence(input_sequence, word2int):\n",
        "    encoded_sequence = [word2int['<SOS>']]\n",
        "    for word in input_sequence.split():\n",
        "        if word in word2int:\n",
        "            encoded_sequence.append(word2int[word])\n",
        "        else:\n",
        "            encoded_sequence.append(word2int['<OUT>'])\n",
        "    return encoded_sequence\n",
        "\n",
        "def decode_output_sequence(output_sequence, int2word):\n",
        "    decoded_sequence = ''\n",
        "    for integer in output_sequence:\n",
        "        if integer > 0:\n",
        "            decoded_sequence += int2word[integer] + ' '\n",
        "    return decoded_sequence\n",
        "\n",
        "# Define a function to generate a response from the chatbot\n",
        "def generate_response(input_text):\n",
        "    # Encode the input sequence\n",
        "    input_sequence = encode_input_sequence(input_text, questionswords2int)\n",
        "    # Pad the input sequence\n",
        "    padded_input_sequence = pad_sequences([input_sequence], maxlen=max_length, padding='post')\n",
        "    # Predict the output sequence\n",
        "    output_sequence = model.predict([padded_input_sequence, np.zeros((1, max_length))])[0]\n",
        "    # Decode the output sequence\n",
        "    decoded_output_sequence = decode_output_sequence(np.argmax(output_sequence, axis=1), answersints2word)\n",
        "    # Remove the <EOS> token from the output sequence\n",
        "    if '<EOS>' in decoded_output_sequence:\n",
        "        decoded_output_sequence = decoded_output_sequence[:decoded_output_sequence.index('<EOS>')]\n",
        "    return decoded_output_sequence\n",
        "\n",
        "# Start the chat\n",
        "while True:\n",
        "    input_text = input('You: ')\n",
        "    if input_text.lower() == 'quit':\n",
        "        break\n",
        "    response = generate_response(input_text)\n",
        "    print('Bot:', response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVlpSoogttjV",
        "outputId": "9e8baf65-769b-4815-c582-daf05b703e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: hi\n",
            "1/1 [==============================] - 1s 834ms/step\n",
            "Bot: mexico riddle do do is is is is <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> \n",
            "You: Hello\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Bot: do <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> \n",
            "You: you know french?\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Bot: <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> \n",
            "You: how is the movie\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Bot: mexico mexico mexico riddle do do is is is is is is <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> <OUT> \n",
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}